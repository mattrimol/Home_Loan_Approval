{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_data = pd.read_csv('hmda_2017_ca_all-records_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling imports\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing, pipeline, decomposition, compose\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import imblearn.under_sampling\n",
    "import imblearn.over_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store this to use later on if we want to dig more into interperetablity of results\n",
    "denials = all_data[['respondent_id', 'denial_reason_name_1', 'denial_reason_name_2', 'denial_reason_name_3']]\n",
    "\n",
    "all_data.drop([\n",
    "                'agency_name',\n",
    "                'agency_code',\n",
    "                'loan_type',\n",
    "                'property_type',\n",
    "                'loan_purpose',\n",
    "                'owner_occupancy',\n",
    "                'preapproval',\n",
    "                'action_taken',\n",
    "                'msamd',\n",
    "                'state_name',\n",
    "                'state_code',\n",
    "                'county_code',\n",
    "                'applicant_ethnicity',\n",
    "                'co_applicant_ethnicity',\n",
    "                'applicant_race_1',\n",
    "                'applicant_race_2',\n",
    "                'applicant_race_3',\n",
    "                'applicant_race_4',\n",
    "                'applicant_race_5',\n",
    "                'co_applicant_race_1',\n",
    "                'co_applicant_race_2',\n",
    "                'co_applicant_race_3',\n",
    "                'co_applicant_race_4',\n",
    "                'co_applicant_race_5',\n",
    "                'applicant_sex',\n",
    "                'co_applicant_sex',\n",
    "                'purchaser_type',\n",
    "                'denial_reason_1',\n",
    "                'denial_reason_2',\n",
    "                'denial_reason_3',\n",
    "                'denial_reason_name_1', #Stored in denials table\n",
    "                'denial_reason_name_2', #Stored in denials table\n",
    "                'denial_reason_name_3', #Stored in denials table\n",
    "                'hoepa_status',\n",
    "                'lien_status',\n",
    "                'edit_status',\n",
    "                'edit_status_name',\n",
    "                'sequence_number',\n",
    "                'application_date_indicator',\n",
    "                'rate_spread'],axis=1, inplace=True)\n",
    "\n",
    "#Only do this if single state and single year\n",
    "all_data.drop(['as_of_year', 'state_abbr'], axis=1, inplace=True)\n",
    "\n",
    "#Shortening some strings\n",
    "all_data.replace('One-to-four family dwelling (other than manufactured housing)',\n",
    "                 '1 to 4 family (excl. manufactured)', inplace=True)\n",
    "all_data.replace('Information not provided by applicant in mail, Internet, or telephone application',\n",
    "                 'Info not provided', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjective Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likely too much complexity to go beyond two race listings\n",
    "all_data.drop(['applicant_race_name_3',\n",
    "               'applicant_race_name_4',\n",
    "               'applicant_race_name_5',\n",
    "               'co_applicant_race_name_3',\n",
    "               'co_applicant_race_name_4',\n",
    "               'co_applicant_race_name_5'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to applications accepted or denied (remove preapprovals and incompletes) and make binary\n",
    "acc_or_den_filter = (all_data['action_taken_name'] == 'Loan originated') |\\\n",
    "                    (all_data['action_taken_name'] == 'Application denied by financial institution')\n",
    "all_data = all_data[acc_or_den_filter]\n",
    "\n",
    "all_data['action_taken_name'].replace('Loan originated', 0, inplace=True)\n",
    "all_data['action_taken_name'].replace('Application denied by financial institution', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping all the entires where we don't have census data\n",
    "all_data = all_data[all_data['population'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping where loan_amount_000s is na\n",
    "all_data = all_data[all_data['loan_amount_000s'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['county_name', #Too many\n",
    "               'census_tract_number', #Too many\n",
    "               'purchaser_type_name',\n",
    "               'hoepa_status_name', #Most are not HOEPA\n",
    "               'lien_status_name', #Probably exclude since vast majority are in one category\n",
    "              ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unreported incomes seem to have similar loan amounts as reported incomes\n",
    "# Droping nulls for now, but consider filling with median. Probably big enough sample to just drop though.\n",
    "all_data = all_data[all_data['applicant_income_000s'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null location for Metropolitan Statistical Area/Metropolitan Division calling 'Other'\n",
    "all_data.loc[all_data['msamd_name'] == np.nan, 'msamd_name'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping multifamily dwellings\n",
    "multifamily = all_data['property_type_name'] == 'Multifamily dwelling'\n",
    "all_data = all_data.drop(all_data[multifamily].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to exclude huge purchases and really small ones\n",
    "# Magic numbers here consider review\n",
    "loan_amt_filter = (all_data['loan_amount_000s'] > 25) &\\\n",
    "                  (all_data['loan_amount_000s'] < 20000)\n",
    "all_data = all_data[loan_amt_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary column, has coapplicant?\n",
    "def has_coapp(row):\n",
    "    if row['co_applicant_sex_name'] == 'No co-applicant':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "all_data['has_coapplicant'] = all_data.apply(lambda row: has_coapp(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 if backed by ANY federal agency\n",
    "def fed_insured(row):\n",
    "    if row['loan_type_name'] == 'Conventional':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "all_data['fed_insured'] = all_data.apply(lambda row: fed_insured(row), axis=1)\n",
    "all_data.drop('loan_type_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_cols(df):\n",
    "    '''Print vertical list so you can paste into excel and make notes'''\n",
    "    for col in df.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_feature(df, feature):\n",
    "    if is_numeric_dtype(df[feature]):\n",
    "        plt.hist(df[feature])\n",
    "    else:\n",
    "        print(df[feature].value_counts())\n",
    "        ((df[feature].value_counts() / len(df[feature]))*100).sort_values().plot(kind = 'barh')\n",
    "        plt.xticks(np.arange(0, 100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_df(df, features_list):\n",
    "    return df[features_list + ['action_taken_name']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in all_data['action_taken_name'].value_counts():\n",
    "    print(count/len(all_data['action_taken_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_feature(all_data, 'action_taken_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get_cols(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose features and make a dataframe with those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1 = ['loan_amount_000s', 'applicant_income_000s', 'hud_median_family_income']\n",
    "model1 = create_model_df(all_data, features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_splits(df):\n",
    "    '''Makes X_train, X_test, y_train_y_test given a data frame with features and action_taken_name as y.\n",
    "    Use this to make sure we don't mess up getting data from the wrong df'''\n",
    "    return train_test_split(df[np.setdiff1d(df.columns, ['action_taken_name'])],\n",
    "                                          df['action_taken_name'],\n",
    "                                          test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr1, X_te1, y_tr1, y_te1 = make_splits(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_columns(X_train, X_test):\n",
    "    num_cols = []\n",
    "    cat_cols = []\n",
    "    \n",
    "    #This is a list that will be the dummy column to drop (it will be the one with the lowest value count)\n",
    "    drop_dummy_cols = []\n",
    "    \n",
    "    # Create the column lists from X_train and then apply the procedure to X_train and X_test\n",
    "    for feature in X_train.columns:\n",
    "        if type(all_data[feature].iloc[0]) == str:\n",
    "            cat_cols.append(feature) # Creating the list of columns that are categorical\n",
    "\n",
    "            # Gets the category name with the lowest count (this will be the dummy column we drop)\n",
    "            min_cat = all_data[feature].value_counts().keys()[-1]\n",
    "\n",
    "            drop_dummy_cols.append(feature + '_' + min_cat) #string convention from pd.get_dummies\n",
    "\n",
    "        else:\n",
    "            num_cols.append(feature) #Columns that will get the standard scaler\n",
    "    \n",
    "    \n",
    "    std_scale = preprocessing.StandardScaler()\n",
    "    for column in X_train.columns:\n",
    "        if column in num_cols:\n",
    "            #standard scaling the numerical columns\n",
    "            X_train[column] = std_scale.fit_transform(np.array(X_train[column]).reshape(-1, 1))\n",
    "            X_test[column] = std_scale.fit_transform(np.array(X_test[column]).reshape(-1, 1))\n",
    "    \n",
    "    #Get dummies for the categoricals and drop the one with the lowest value count\n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_test = pd.get_dummies(X_test)\n",
    "    X_train.drop(drop_dummy_cols, axis=1, inplace=True)\n",
    "    X_test.drop(drop_dummy_cols, axis=1, inplace=True)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(model1.sample(frac=0.3, replace=True, random_state=1), hue='action_taken_name');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sorted(metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% is denials (action_taken_name = 1)\n",
    "# 80% is loan originated\n",
    "def under_sample_bal(X_train, y_train):\n",
    "    '''pass in training data and return X_train_rs, y_train_rs'''\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    ratio = {1 : int(n_pos), 0 : int(n_neg/4)} \n",
    "\n",
    "    ROS = imblearn.under_sampling.RandomUnderSampler(sampling_strategy = ratio, random_state=42) \n",
    "    return ROS.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_c(X_train, y_train):\n",
    "    basic_model = LogisticRegression()\n",
    "    c_vals_grid = [{'C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 10.0, 100, 1000, 10000],}]\n",
    "    # Cross validates to find optimal C\n",
    "    \n",
    "    #Grid search object that is looking for best C value\n",
    "    basic_model = GridSearchCV(estimator=basic_model, param_grid=c_vals_grid, scoring='roc_auc', cv=5)\n",
    "    \n",
    "    # Preprcess - Since the function is made to return processed df for train and test we are just dropping it some\n",
    "    # dummy data so it doesn't error.\n",
    "    X_tr_proc, dummy_df = preprocess_columns(X_train, X_train)\n",
    "    # Resample balancing\n",
    "    X_tr_rs, y_tr_rs = under_sample_bal(X_tr_proc, y_train)\n",
    "    \n",
    "    # Fit the model with processed and resampled training data\n",
    "    basic_model.fit(X_tr_rs, y_tr_rs)\n",
    "    \n",
    "    #Return optimal C\n",
    "    return basic_model.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take in unprocessed X_train, y_train\n",
    "def cross_validate(X_train, y_train):\n",
    "    C = find_c(X_train, y_train)\n",
    "    scores = {'accuracy': [], 'recall': [], 'roc_auc': [], 'f1': [], 'precision': []}\n",
    "    sfk = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    for train_index, test_index in sfk.split(X_train, y_train):\n",
    "        #create a train/val set for a split\n",
    "        X_tr, X_val = X_train.iloc[train_index].copy(), X_train.iloc[test_index].copy()\n",
    "        y_tr, y_val = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "        \n",
    "        #preprocess X sets\n",
    "        X_tr_proc, X_val_proc = preprocess_columns(X_tr, X_val)\n",
    "        #Balance pos/neg class with undersampling\n",
    "        X_tr_rs, y_tr_rs = under_sample_bal(X_tr_proc, y_tr)\n",
    "        #fit to train\n",
    "        model = LogisticRegression(C=C)\n",
    "        model.fit(X_tr_rs, y_tr_rs)\n",
    "        #score on val\n",
    "        y_pred = model.predict(X_val_proc)\n",
    "        \n",
    "        # Score test data\n",
    "        accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "        scores['accuracy'].append(accuracy)\n",
    "        \n",
    "        recall = metrics.recall_score(y_val, y_pred)\n",
    "        scores['recall'].append(recall)\n",
    "        \n",
    "        roc_auc = metrics.roc_auc_score(y_val, y_pred)\n",
    "        scores['roc_auc'].append(roc_auc)       \n",
    "        \n",
    "        f1 = metrics.f1_score(y_val, y_pred)\n",
    "        scores['f1'].append(f1)\n",
    "        \n",
    "        precision = metrics.precision_score(y_val, y_pred)\n",
    "        scores['precision'].append(precision)\n",
    "        \n",
    "    print('Means:')\n",
    "    for metric in scores:\n",
    "        mean = np.mean(scores[metric])\n",
    "        print('{:<20s}{:>15.2f}'.format(metric, mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_validate(X_tr2, y_tr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test(X_train, X_test, y_train, y_test):\n",
    "    C = find_c(X_train, y_train)\n",
    "    model = LogisticRegression(C=C)\n",
    "    \n",
    "    #preprocess X sets\n",
    "    X_train_proc, X_test_proc = preprocess_columns(X_train, X_test)\n",
    "    #Balance pos/neg class with undersampling\n",
    "    X_train_rs, y_train_rs = under_sample_bal(X_train_proc, y_train)\n",
    "    #fit to train\n",
    "    model = LogisticRegression(C=C)\n",
    "    model.fit(X_train_rs, y_train_rs)\n",
    "    #generate predictions\n",
    "    y_pred = model.predict(X_test_proc)\n",
    "    \n",
    "    # Now that everything is transformed use:\n",
    "        # X_train_rs\n",
    "        # X_test_proc\n",
    "        # y_train_rs\n",
    "        # y_test       \n",
    "\n",
    "    # Score test data\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "    print('Important:')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Recall: {recall}')\n",
    "\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test_proc)[:,1])\n",
    "    plt.plot(fpr, tpr,lw=2)\n",
    "    plt.plot([0,1],[0,1],c='violet',ls='--')\n",
    "    plt.xlim([-0.05,1.05])\n",
    "    plt.ylim([-0.05,1.05])\n",
    "\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve');\n",
    "    print(\"ROC AUC score = \", metrics.roc_auc_score(y_test, model.predict_proba(X_test_proc)[:,1]))\n",
    "    \n",
    "    #for feature, coef in X_train_rs.columns, model.coef_:\n",
    "        #print('{:<20s}{:>15.2f}'.format(feature, coef))\n",
    "    print(model.coef_)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print('\\nLess Important:')\n",
    "    print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(X_tr2, X_te2, y_tr2, y_te2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
